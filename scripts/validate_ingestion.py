#!/usr/bin/env python3
"""
Data Ingestion Validation Script - Flourish Skills Tracker

Validates that all mock data entries were successfully ingested and
assessments were properly generated by the AI inference pipeline.

Usage:
    python scripts/validate_ingestion.py
"""

import sys
import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Tuple

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

import psycopg2
from psycopg2.extras import RealDictCursor

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Database connection
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://flourish_admin:secure_password_123@localhost:5433/skills_tracker_db")


def get_db_connection():
    """Get database connection"""
    try:
        conn = psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor)
        return conn
    except Exception as e:
        logger.error(f"Database connection failed: {e}")
        sys.exit(1)


def run_validation_checks() -> Dict:
    """
    Run comprehensive validation checks on ingested data

    Returns:
        Dictionary with validation results
    """
    logger.info("=" * 60)
    logger.info("DATA INGESTION VALIDATION")
    logger.info("=" * 60)

    conn = get_db_connection()
    cur = conn.cursor()

    results = {
        "checks": [],
        "passed": 0,
        "failed": 0,
        "warnings": 0
    }

    # Check 1: Total data entries
    logger.info("\n[CHECK 1] Verifying data entry count...")
    cur.execute("SELECT COUNT(*) as count FROM data_entries")
    data_entry_count = cur.fetchone()['count']
    expected_entries = 32  # Modified scope from Shard 2

    if data_entry_count == expected_entries:
        logger.info(f"‚úÖ PASS: Found {data_entry_count} data entries (expected {expected_entries})")
        results['checks'].append({"name": "Data entry count", "status": "PASS", "details": f"{data_entry_count}/{expected_entries}"})
        results['passed'] += 1
    else:
        logger.warning(f"‚ö†Ô∏è  WARNING: Found {data_entry_count} data entries (expected {expected_entries})")
        results['checks'].append({"name": "Data entry count", "status": "WARNING", "details": f"{data_entry_count}/{expected_entries}"})
        results['warnings'] += 1

    # Check 2: Total assessments
    logger.info("\n[CHECK 2] Verifying assessment generation...")
    cur.execute("SELECT COUNT(*) as count FROM assessments")
    assessment_count = cur.fetchone()['count']

    # The AI generates 1-2 assessments per entry on average (being selective about confident assessments)
    # With 32 entries, we expect at least 20-30 assessments minimum
    min_expected = max(20, int(data_entry_count * 0.6))  # At least 60% of entries
    max_expected = data_entry_count * 3  # Up to 3 per entry

    if min_expected <= assessment_count <= max_expected:
        logger.info(f"‚úÖ PASS: Found {assessment_count} assessments (expected {min_expected}-{max_expected})")
        results['checks'].append({"name": "Assessment count", "status": "PASS", "details": f"{assessment_count} assessments"})
        results['passed'] += 1
    elif assessment_count < min_expected:
        logger.warning(f"‚ö†Ô∏è  WARNING: Found {assessment_count} assessments (expected {min_expected}-{max_expected})")
        results['checks'].append({"name": "Assessment count", "status": "WARNING", "details": f"{assessment_count} assessments"})
        results['warnings'] += 1
    else:
        logger.warning(f"‚ö†Ô∏è  WARNING: Found {assessment_count} assessments (more than expected {max_expected})")
        results['checks'].append({"name": "Assessment count", "status": "WARNING", "details": f"{assessment_count} assessments"})
        results['warnings'] += 1

    # Check 3: Assessments per student
    logger.info("\n[CHECK 3] Verifying assessments per student...")
    cur.execute("""
        SELECT student_id, COUNT(*) as count
        FROM assessments
        GROUP BY student_id
        ORDER BY student_id
    """)
    student_assessments = cur.fetchall()

    all_students_have_assessments = len(student_assessments) == 4  # 4 students total
    for row in student_assessments:
        logger.info(f"  {row['student_id']}: {row['count']} assessments")

    if all_students_have_assessments and all(s['count'] > 0 for s in student_assessments):
        logger.info(f"‚úÖ PASS: All 4 students have assessments")
        results['checks'].append({"name": "Student coverage", "status": "PASS", "details": "All students assessed"})
        results['passed'] += 1
    else:
        logger.error(f"‚ùå FAIL: Not all students have assessments")
        results['checks'].append({"name": "Student coverage", "status": "FAIL", "details": f"{len(student_assessments)}/4 students"})
        results['failed'] += 1

    # Check 4: Skills assessed
    logger.info("\n[CHECK 4] Verifying skill coverage...")
    cur.execute("""
        SELECT DISTINCT skill_name, skill_category, COUNT(*) as count
        FROM assessments
        GROUP BY skill_name, skill_category
        ORDER BY skill_category, skill_name
    """)
    skills = cur.fetchall()

    logger.info(f"  Found {len(skills)} unique skills assessed:")
    for skill in skills:
        logger.info(f"    - {skill['skill_name']} ({skill['skill_category']}): {skill['count']} assessments")

    # We expect at least 6-8 different skills to be covered across all categories
    # (There are 17 total skills, but AI will only assess what it can confidently observe)
    min_skill_coverage = 6
    if len(skills) >= min_skill_coverage:
        logger.info(f"‚úÖ PASS: {len(skills)} skills assessed (expected ‚â•{min_skill_coverage})")
        results['checks'].append({"name": "Skill coverage", "status": "PASS", "details": f"{len(skills)} skills"})
        results['passed'] += 1
    else:
        logger.warning(f"‚ö†Ô∏è  WARNING: Only {len(skills)} skills assessed (expected ‚â•{min_skill_coverage})")
        results['checks'].append({"name": "Skill coverage", "status": "WARNING", "details": f"{len(skills)} skills"})
        results['warnings'] += 1

    # Check 5: Confidence scores
    logger.info("\n[CHECK 5] Verifying confidence scores...")
    cur.execute("""
        SELECT
            MIN(confidence_score) as min_score,
            MAX(confidence_score) as max_score,
            AVG(confidence_score) as avg_score,
            COUNT(CASE WHEN confidence_score < 0.5 THEN 1 END) as below_threshold
        FROM assessments
    """)
    confidence_stats = cur.fetchone()

    logger.info(f"  Min confidence: {confidence_stats['min_score']:.2f}")
    logger.info(f"  Max confidence: {confidence_stats['max_score']:.2f}")
    logger.info(f"  Avg confidence: {confidence_stats['avg_score']:.2f}")
    logger.info(f"  Below 0.5: {confidence_stats['below_threshold']}")

    if confidence_stats['min_score'] >= 0.5 and confidence_stats['max_score'] <= 1.0:
        logger.info(f"‚úÖ PASS: All confidence scores in valid range [0.5, 1.0]")
        results['checks'].append({"name": "Confidence scores", "status": "PASS", "details": f"Avg: {confidence_stats['avg_score']:.2f}"})
        results['passed'] += 1
    else:
        logger.error(f"‚ùå FAIL: Confidence scores out of range")
        results['checks'].append({"name": "Confidence scores", "status": "FAIL", "details": "Out of range"})
        results['failed'] += 1

    # Check 6: Level distribution
    logger.info("\n[CHECK 6] Verifying level distribution...")
    cur.execute("""
        SELECT level, COUNT(*) as count
        FROM assessments
        GROUP BY level
        ORDER BY
            CASE level
                WHEN 'Emerging' THEN 1
                WHEN 'E' THEN 1
                WHEN 'Developing' THEN 2
                WHEN 'D' THEN 2
                WHEN 'Proficient' THEN 3
                WHEN 'P' THEN 3
                WHEN 'Advanced' THEN 4
                WHEN 'A' THEN 4
            END
    """)
    level_distribution = cur.fetchall()

    logger.info(f"  Level distribution:")
    for row in level_distribution:
        logger.info(f"    {row['level']}: {row['count']} assessments ({row['count']/assessment_count*100:.1f}%)")

    # Check if we have reasonable distribution (at least 2 different levels)
    if len(level_distribution) >= 2:
        logger.info(f"‚úÖ PASS: Assessments span multiple proficiency levels")
        results['checks'].append({"name": "Level distribution", "status": "PASS", "details": f"{len(level_distribution)} levels"})
        results['passed'] += 1
    else:
        logger.warning(f"‚ö†Ô∏è  WARNING: All assessments at same level")
        results['checks'].append({"name": "Level distribution", "status": "WARNING", "details": "Limited variation"})
        results['warnings'] += 1

    # Check 7: Data quality (non-empty justifications)
    logger.info("\n[CHECK 7] Verifying assessment data quality...")
    cur.execute("""
        SELECT
            COUNT(CASE WHEN justification IS NULL OR justification = '' THEN 1 END) as empty_justifications,
            COUNT(CASE WHEN source_quote IS NULL OR source_quote = '' THEN 1 END) as empty_quotes,
            COUNT(*) as total
        FROM assessments
    """)
    quality_stats = cur.fetchone()

    logger.info(f"  Empty justifications: {quality_stats['empty_justifications']}/{quality_stats['total']}")
    logger.info(f"  Empty source quotes: {quality_stats['empty_quotes']}/{quality_stats['total']}")

    if quality_stats['empty_justifications'] == 0 and quality_stats['empty_quotes'] == 0:
        logger.info(f"‚úÖ PASS: All assessments have complete data")
        results['checks'].append({"name": "Data quality", "status": "PASS", "details": "All fields populated"})
        results['passed'] += 1
    else:
        logger.error(f"‚ùå FAIL: Some assessments have missing data")
        results['checks'].append({"name": "Data quality", "status": "FAIL", "details": "Missing fields"})
        results['failed'] += 1

    # Check 8: Teacher corrections
    logger.info("\n[CHECK 8] Checking teacher corrections...")
    cur.execute("SELECT COUNT(*) as count FROM teacher_corrections")
    correction_count = cur.fetchone()['count']

    logger.info(f"  Found {correction_count} teacher corrections")

    if correction_count >= 0:
        logger.info(f"‚úÖ INFO: Teacher corrections table operational ({correction_count} corrections)")
        results['checks'].append({"name": "Corrections table", "status": "PASS", "details": f"{correction_count} corrections"})
        results['passed'] += 1

    # Check 9: Targets
    logger.info("\n[CHECK 9] Checking skill targets...")
    cur.execute("SELECT COUNT(*) as count FROM skill_targets")
    target_count = cur.fetchone()['count']

    logger.info(f"  Found {target_count} skill targets")

    if target_count >= 0:
        logger.info(f"‚úÖ INFO: Skill targets table operational ({target_count} targets)")
        results['checks'].append({"name": "Targets table", "status": "PASS", "details": f"{target_count} targets"})
        results['passed'] += 1

    # Check 10: Badges
    logger.info("\n[CHECK 10] Checking badges...")
    cur.execute("SELECT COUNT(*) as count FROM badges")
    badge_count = cur.fetchone()['count']

    logger.info(f"  Found {badge_count} badges")

    if badge_count >= 0:
        logger.info(f"‚úÖ INFO: Badges table operational ({badge_count} badges)")
        results['checks'].append({"name": "Badges table", "status": "PASS", "details": f"{badge_count} badges"})
        results['passed'] += 1

    cur.close()
    conn.close()

    return results


def print_summary(results: Dict):
    """Print validation summary"""
    logger.info("\n" + "=" * 60)
    logger.info("VALIDATION SUMMARY")
    logger.info("=" * 60)

    total_checks = results['passed'] + results['failed'] + results['warnings']
    logger.info(f"Total checks: {total_checks}")
    logger.info(f"‚úÖ Passed: {results['passed']}")
    logger.info(f"‚ö†Ô∏è  Warnings: {results['warnings']}")
    logger.info(f"‚ùå Failed: {results['failed']}")

    pass_rate = (results['passed'] / total_checks * 100) if total_checks > 0 else 0
    logger.info(f"\nPass rate: {pass_rate:.1f}%")

    if results['failed'] == 0:
        logger.info("\nüéâ VALIDATION PASSED - All critical checks successful!")
        logger.info("=" * 60)
        return True
    else:
        logger.error("\n‚ùå VALIDATION FAILED - Some critical checks failed")
        logger.error("=" * 60)
        return False


def main():
    """Main entry point"""
    try:
        results = run_validation_checks()
        success = print_summary(results)

        # Save results to file
        output_file = Path(__file__).parent / "validation_results.json"
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        logger.info(f"\nResults saved to {output_file}")

        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        logger.info("\n\nValidation interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Validation error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
